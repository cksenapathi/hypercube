Create a living network that learns continually
there must be a locality-encoding for blanket states
the agent "network" must learn a simplex hamiltonian structure
the simplex hamiltonian represents action density flows
  !!! How action density changes in tandem across topological spaces !!!
the simplex hamiltonian is in a valid state if:
  - represents all transformations, observed and inferred
  - balances the accuracy and confidence with data encoding
  - incoming action is perfectly balanced with outgoing action

could potentially have an error buffer (to-be-learned stuff)
real-time context is based on local action adaptation
sleep/downtime might (prob will) be needed for structural changes
should be mostly encoded to low-D vector space of diff forms

topological action wavelets (spikes) are sufficient for computation
topology structure encode which wavelets (diff forms) act on density

blanket states (obs and act spaces) are also models as top. vec space
encoding and decoding included as map bs TVS

for each transformation between blanket states, k-simplex
there is corresponding (k+1)-simplex for hdv transformation

each grade of diff forms has their own density
density over diff 1 forms is one step space-time estimate
density over diff k-forms is k-step space time estimates
  - gets to multisteps process predictions

^^^
Not entirely true
counting measure e.g., how many total activations, is the total density, sums to 1, for each time step
how this measure/density shifts is neural dynamics
how neural dynamics shape behavior comes from connectivity


if each simplex is a vector field, that means its the gradient of some conformal hamiltonian

how much is the total dynamics hamiltonian just a composition of 1d Hams?
when are higher dimensional dynamical systems required?

there exists a conformal mapping from basic hamiltonian system to one of any arbitrary shape -- loop shaping

k-simplex is k-dim vector field

each conformal hamiltonian is essentially infinite dimensional
however, with kernel trick, each infinite dimensional optimization
  can be made finite dimensional with local basis
basis at each event is a set of compactly supported 

questions:
  - how are higher order diff forms/simplicies/effects probabilistically
incorporated over time
    - directed connection strength bw graded diff forms
    - positive strength




